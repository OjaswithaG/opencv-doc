#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üéØ Alƒ±≈ütƒ±rma 1 √á√∂z√ºm√º: √áok Sƒ±nƒ±flƒ± G√∂r√ºnt√º Sƒ±nƒ±flandƒ±rma
=======================================================

Bu dosya alƒ±≈ütƒ±rma 1'in tam √ß√∂z√ºm√ºn√º i√ßerir.
T√ºm makine √∂ƒürenmesi algoritmalarƒ±nƒ± kullanarak kapsamlƒ± bir
g√∂r√ºnt√º sƒ±nƒ±flandƒ±rma sistemi implementasyonu.

Yazan: Eren Terzi
Tarih: 2024
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, LabelEncoder
import time
import os
import pickle
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class ImageClassificationProject:
    """G√∂r√ºnt√º sƒ±nƒ±flandƒ±rma projesi ana sƒ±nƒ±fƒ±"""
    
    def __init__(self, n_classes=5):
        self.n_classes = n_classes
        self.class_names = [f"Sinif_{i}" for i in range(n_classes)]
        self.dataset = []
        self.labels = []
        self.models = {}
        self.scaler = StandardScaler()
        self.feature_dim = 32
        
        print(f"üéØ G√∂r√ºnt√º Sƒ±nƒ±flandƒ±rma Projesi ba≈ülatƒ±ldƒ± ({n_classes} sƒ±nƒ±f)")
    
    def set_class_names(self, names):
        """Sƒ±nƒ±f isimlerini ayarla"""
        if len(names) != self.n_classes:
            raise ValueError(f"Sƒ±nƒ±f sayƒ±sƒ± {self.n_classes} olmalƒ±, {len(names)} verildi")
        self.class_names = names
        print(f"üìù Sƒ±nƒ±f isimleri g√ºncellendi: {names}")

class DataCollector:
    """Veri toplama sƒ±nƒ±fƒ±"""
    
    def __init__(self, project):
        self.project = project
        self.current_class = 0
        self.samples_per_class = [0] * project.n_classes
        
    def collect_data_interactive(self):
        """ƒ∞nteraktif veri toplama"""
        print("\nüì∏ ƒ∞nteraktif Veri Toplama Modu")
        print("=" * 35)
        print("Kontroller:")
        print("  0-4: Sƒ±nƒ±f se√ßimi")
        print("  SPACE: √ñrnek kaydet")
        print("  S: ƒ∞statistikleri g√∂ster")
        print("  ESC: √áƒ±kƒ±≈ü")
        
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("‚ùå Webcam a√ßƒ±lamadƒ±!")
            return False
        
        while True:
            ret, frame = cap.read()
            if not ret:
                continue
            
            # UI bilgilerini frame √ºzerine √ßiz
            display_frame = frame.copy()
            
            # Mevcut sƒ±nƒ±f bilgisi
            cv2.putText(display_frame, f'Mevcut Sinif: {self.project.class_names[self.current_class]}', 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            
            # Sƒ±nƒ±f ba≈üƒ±na √∂rnek sayƒ±sƒ±
            y_pos = 60
            for i, count in enumerate(self.samples_per_class):
                color = (0, 255, 0) if i == self.current_class else (255, 255, 255)
                cv2.putText(display_frame, f'{self.project.class_names[i]}: {count}', 
                           (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                y_pos += 25
            
            # Toplam √∂rnek sayƒ±sƒ±
            total_samples = sum(self.samples_per_class)
            cv2.putText(display_frame, f'Toplam: {total_samples} ornek', 
                       (10, y_pos + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
            
            # Kontrol bilgileri
            cv2.putText(display_frame, '0-4:Sinif SPACE:Kaydet S:Stat ESC:Cikis', 
                       (10, frame.shape[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            # √áer√ßeve √ßiz (mevcut sƒ±nƒ±f i√ßin)
            cv2.rectangle(display_frame, (50, 50), (frame.shape[1] - 50, frame.shape[0] - 100), 
                         (0, 255, 255), 2)
            
            cv2.imshow('Veri Toplama', display_frame)
            
            key = cv2.waitKey(1) & 0xFF
            
            if key == 27:  # ESC
                break
            elif key >= ord('0') and key <= ord('4'):  # Sƒ±nƒ±f se√ßimi
                new_class = key - ord('0')
                if new_class < self.project.n_classes:
                    self.current_class = new_class
                    print(f"üéØ Sƒ±nƒ±f deƒüi≈ütirildi: {self.project.class_names[self.current_class]}")
                    
            elif key == ord(' '):  # SPACE - √ñrnek kaydet
                if self.save_example(frame, self.current_class):
                    self.samples_per_class[self.current_class] += 1
                    print(f"‚úÖ {self.project.class_names[self.current_class]} - {self.samples_per_class[self.current_class]}. √∂rnek kaydedildi")
                    
            elif key == ord('s') or key == ord('S'):  # ƒ∞statistik
                self.show_statistics()
        
        cap.release()
        cv2.destroyAllWindows()
        
        return len(self.project.dataset) > 0
    
    def save_example(self, frame, class_id):
        """√ñrnek kaydetme"""
        try:
            # Frame'i i≈üle
            roi = frame[50:frame.shape[0]-100, 50:frame.shape[1]-50]  # ROI al
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)  # Gri tonlama
            resized = cv2.resize(gray, (32, 32))  # 32x32'ye resize
            
            # √ñzellik √ßƒ±kar
            features = FeatureExtractor.extract_comprehensive_features(resized)
            
            # Dataset'e ekle
            self.project.dataset.append(features)
            self.project.labels.append(class_id)
            
            return True
        except Exception as e:
            print(f"‚ùå √ñrnek kaydetme hatasƒ±: {e}")
            return False
    
    def show_statistics(self):
        """Veri istatistiklerini g√∂ster"""
        print("\nüìä Veri ƒ∞statistikleri:")
        print("-" * 25)
        total = sum(self.samples_per_class)
        for i, count in enumerate(self.samples_per_class):
            percentage = (count / total * 100) if total > 0 else 0
            print(f"  {self.project.class_names[i]}: {count} √∂rnek ({percentage:.1f}%)")
        print(f"  Toplam: {total} √∂rnek")
        
        # Minimum gereksinim kontrol√º
        min_required = 50
        insufficient_classes = [i for i, count in enumerate(self.samples_per_class) if count < min_required]
        if insufficient_classes:
            print(f"\n‚ö†Ô∏è Yetersiz veri sƒ±nƒ±flarƒ± (min {min_required}):")
            for i in insufficient_classes:
                print(f"  {self.project.class_names[i]}: {self.samples_per_class[i]}/{min_required}")

class FeatureExtractor:
    """√ñzellik √ßƒ±karma sƒ±nƒ±fƒ±"""
    
    @staticmethod
    def extract_comprehensive_features(image):
        """Kapsamlƒ± √∂zellik √ßƒ±karma - 32 boyutlu vekt√∂r"""
        features = []
        
        # 1. Histogram √ñzellikleri (16 boyut)
        hist_features = FeatureExtractor.extract_histogram_features(image, bins=16)
        features.extend(hist_features)
        
        # 2. ƒ∞statistiksel √ñzellikler (4 boyut)
        stat_features = FeatureExtractor.extract_statistical_features(image)
        features.extend(stat_features)
        
        # 3. Kenar √ñzellikleri (2 boyut)
        edge_features = FeatureExtractor.extract_edge_features(image)
        features.extend(edge_features)
        
        # 4. Doku √ñzellikleri - LBP (10 boyut)
        lbp_features = FeatureExtractor.extract_lbp_features(image, n_bins=10)
        features.extend(lbp_features)
        
        return np.array(features, dtype=np.float32)
    
    @staticmethod
    def extract_histogram_features(image, bins=16):
        """Histogram √∂zelliklerini √ßƒ±kar"""
        hist = cv2.calcHist([image], [0], None, [bins], [0, 256])
        hist = hist.flatten()
        # Normalize et
        hist = hist / (np.sum(hist) + 1e-8)  # Sƒ±fƒ±ra b√∂l√ºnme korumasƒ±
        return hist.tolist()
    
    @staticmethod
    def extract_statistical_features(image):
        """ƒ∞statistiksel √∂zellikler"""
        mean_val = np.mean(image)
        std_val = np.std(image)
        min_val = np.min(image)
        max_val = np.max(image)
        
        return [mean_val, std_val, min_val, max_val]
    
    @staticmethod
    def extract_edge_features(image):
        """Kenar √∂zelliklerini √ßƒ±kar"""
        # Canny edge detection
        edges = cv2.Canny(image, 50, 150)
        edge_density = np.sum(edges > 0) / (image.shape[0] * image.shape[1])
        
        # Sobel gradyan
        grad_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)
        gradient_magnitude = np.mean(np.sqrt(grad_x**2 + grad_y**2))
        
        return [edge_density, gradient_magnitude / 255.0]  # Normalize
    
    @staticmethod
    def extract_lbp_features(image, radius=1, n_points=8, n_bins=10):
        """LBP doku √∂zellikleri"""
        # Basit LBP implementasyonu
        height, width = image.shape
        lbp_image = np.zeros((height, width), dtype=np.uint8)
        
        for i in range(radius, height - radius):
            for j in range(radius, width - radius):
                center = image[i, j]
                pattern = 0
                
                # 8 kom≈üu pikseli kontrol et
                for k in range(n_points):
                    angle = 2 * np.pi * k / n_points
                    x = i + radius * np.cos(angle)
                    y = j + radius * np.sin(angle)
                    
                    # Bilinear interpolation
                    x1, y1 = int(x), int(y)
                    x2, y2 = min(x1 + 1, height - 1), min(y1 + 1, width - 1)
                    
                    if x1 < height and y1 < width:
                        neighbor = image[x1, y1]
                        if neighbor >= center:
                            pattern |= (1 << k)
                
                lbp_image[i, j] = pattern
        
        # LBP histogramƒ± olu≈ütur
        hist, _ = np.histogram(lbp_image.ravel(), bins=n_bins, range=[0, 2**n_points])
        hist = hist.astype(np.float32)
        # Normalize et
        hist = hist / (np.sum(hist) + 1e-8)
        
        return hist.tolist()

class ModelTrainer:
    """Model eƒüitim sƒ±nƒ±fƒ±"""
    
    def __init__(self, project):
        self.project = project
        self.results = {}
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
    
    def prepare_data(self):
        """Veriyi eƒüitim i√ßin hazƒ±rla"""
        if len(self.project.dataset) == 0:
            raise ValueError("Veri seti bo≈ü!")
        
        print(f"üìä Veri hazƒ±rlanƒ±yor: {len(self.project.dataset)} √∂rnek")
        
        X = np.array(self.project.dataset)
        y = np.array(self.project.labels)
        
        # Train/Test split - stratified
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"üîÑ Veri b√∂l√ºnmesi: Train={len(self.X_train)}, Test={len(self.X_test)}")
        
        # √ñzellik normalizasyonu
        self.X_train = self.project.scaler.fit_transform(self.X_train).astype(np.float32)
        self.X_test = self.project.scaler.transform(self.X_test).astype(np.float32)
        
        print("‚úÖ Veri normalizasyonu tamamlandƒ±")
        
        return self.X_train, self.X_test, self.y_train, self.y_test
    
    def train_knn_models(self, X_train, X_test, y_train, y_test):
        """k-NN modellerini eƒüit"""
        print("\nüéØ k-NN Modelleri Eƒüitiliyor...")
        
        k_values = [3, 5, 7]
        
        for k in k_values:
            print(f"  k={k} test ediliyor...")
            
            # OpenCV k-NN
            knn = cv2.ml.KNearest_create()
            knn.setDefaultK(k)
            knn.setIsClassifier(True)
            
            # Eƒüitim
            start_time = time.time()
            knn.train(X_train, cv2.ml.ROW_SAMPLE, y_train.astype(np.int32))
            training_time = time.time() - start_time
            
            # Test
            start_time = time.time()
            _, predictions = knn.predict(X_test)
            prediction_time = time.time() - start_time
            
            predictions = predictions.flatten().astype(np.int32)
            
            # Deƒüerlendirme
            accuracy = accuracy_score(y_test, predictions)
            report = classification_report(y_test, predictions, output_dict=True)
            
            model_name = f"k-NN (k={k})"
            self.results[model_name] = {
                'model': knn,
                'accuracy': accuracy,
                'precision': report['macro avg']['precision'],
                'recall': report['macro avg']['recall'],
                'f1_score': report['macro avg']['f1-score'],
                'training_time': training_time * 1000,  # ms
                'prediction_time': prediction_time * 1000,  # ms
                'predictions': predictions
            }
            
            print(f"    Doƒüruluk: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    def train_svm_models(self, X_train, X_test, y_train, y_test):
        """SVM modellerini eƒüit"""
        print("\n‚öñÔ∏è SVM Modelleri Eƒüitiliyor...")
        
        svm_configs = [
            {'kernel': cv2.ml.SVM_LINEAR, 'name': 'Linear'},
            {'kernel': cv2.ml.SVM_RBF, 'name': 'RBF', 'C': 10.0, 'gamma': 0.1},
            {'kernel': cv2.ml.SVM_POLY, 'name': 'Polynomial', 'C': 1.0, 'degree': 3}
        ]
        
        for config in svm_configs:
            print(f"  {config['name']} kernel test ediliyor...")
            
            # OpenCV SVM
            svm = cv2.ml.SVM_create()
            svm.setType(cv2.ml.SVM_C_SVC)
            svm.setKernel(config['kernel'])
            
            if 'C' in config:
                svm.setC(config['C'])
            if 'gamma' in config:
                svm.setGamma(config['gamma'])
            if 'degree' in config:
                svm.setDegree(config['degree'])
            
            # Eƒüitim
            start_time = time.time()
            svm.train(X_train, cv2.ml.ROW_SAMPLE, y_train.astype(np.int32))
            training_time = time.time() - start_time
            
            # Test
            start_time = time.time()
            _, predictions = svm.predict(X_test)
            prediction_time = time.time() - start_time
            
            predictions = predictions.flatten().astype(np.int32)
            
            # Deƒüerlendirme
            accuracy = accuracy_score(y_test, predictions)
            report = classification_report(y_test, predictions, output_dict=True)
            
            model_name = f"SVM ({config['name']})"
            self.results[model_name] = {
                'model': svm,
                'accuracy': accuracy,
                'precision': report['macro avg']['precision'],
                'recall': report['macro avg']['recall'],
                'f1_score': report['macro avg']['f1-score'],
                'training_time': training_time * 1000,
                'prediction_time': prediction_time * 1000,
                'predictions': predictions
            }
            
            print(f"    Doƒüruluk: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    def train_ann_models(self, X_train, X_test, y_train, y_test):
        """ANN modellerini eƒüit"""
        print("\nüß† ANN Modelleri Eƒüitiliyor...")
        
        architectures = [
            ([32], "32"),
            ([64], "64"), 
            ([32, 16], "32-16"),
            ([64, 32], "64-32")
        ]
        
        for hidden_layers, arch_name in architectures:
            print(f"  Mimari {arch_name} test ediliyor...")
            
            # OpenCV ANN
            ann = cv2.ml.ANN_MLP_create()
            
            # Aƒü mimarisi
            layer_sizes = [X_train.shape[1]] + hidden_layers + [self.project.n_classes]
            ann.setLayerSizes(np.array(layer_sizes, dtype=np.int32))
            
            # Aktivasyon fonksiyonu
            ann.setActivationFunction(cv2.ml.ANN_MLP_SIGMOID_SYM)
            
            # Eƒüitim parametreleri
            ann.setTrainMethod(cv2.ml.ANN_MLP_BACKPROP)
            ann.setBackpropMomentumScale(0.9)
            ann.setBackpropWeightScale(0.01)
            
            # Durdurma kriterleri
            ann.setTermCriteria((cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS, 500, 1e-6))
            
            # Etiketleri one-hot encode et
            y_train_encoded = np.zeros((len(y_train), self.project.n_classes), dtype=np.float32)
            for i, label in enumerate(y_train):
                y_train_encoded[i, label] = 1.0
            
            # Eƒüitim
            start_time = time.time()
            success = ann.train(X_train, cv2.ml.ROW_SAMPLE, y_train_encoded)
            training_time = time.time() - start_time
            
            if not success:
                print(f"    ‚ùå {arch_name} eƒüitimi ba≈üarƒ±sƒ±z!")
                continue
            
            # Test
            start_time = time.time()
            _, raw_predictions = ann.predict(X_test)
            prediction_time = time.time() - start_time
            
            predictions = np.argmax(raw_predictions, axis=1).astype(np.int32)
            
            # Deƒüerlendirme
            accuracy = accuracy_score(y_test, predictions)
            report = classification_report(y_test, predictions, output_dict=True)
            
            model_name = f"ANN ({arch_name})"
            self.results[model_name] = {
                'model': ann,
                'accuracy': accuracy,
                'precision': report['macro avg']['precision'],
                'recall': report['macro avg']['recall'],
                'f1_score': report['macro avg']['f1-score'],
                'training_time': training_time * 1000,
                'prediction_time': prediction_time * 1000,
                'predictions': predictions
            }
            
            print(f"    Doƒüruluk: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    def train_tree_models(self, X_train, X_test, y_train, y_test):
        """Aƒüa√ß modelleri eƒüit"""
        print("\nüå≥ Aƒüa√ß Modelleri Eƒüitiliyor...")
        
        # Decision Tree
        depths = [5, 10, 15]
        for depth in depths:
            print(f"  Decision Tree (depth={depth}) test ediliyor...")
            
            dt = cv2.ml.DTrees_create()
            dt.setMaxDepth(depth)
            dt.setMinSampleCount(2)
            dt.setCVFolds(0)
            
            # Eƒüitim
            start_time = time.time()
            dt.train(X_train, cv2.ml.ROW_SAMPLE, y_train.astype(np.int32))
            training_time = time.time() - start_time
            
            # Test
            start_time = time.time()
            _, predictions = dt.predict(X_test)
            prediction_time = time.time() - start_time
            
            predictions = predictions.flatten().astype(np.int32)
            
            # Deƒüerlendirme
            accuracy = accuracy_score(y_test, predictions)
            report = classification_report(y_test, predictions, output_dict=True)
            
            model_name = f"Decision Tree (depth={depth})"
            self.results[model_name] = {
                'model': dt,
                'accuracy': accuracy,
                'precision': report['macro avg']['precision'],
                'recall': report['macro avg']['recall'],
                'f1_score': report['macro avg']['f1-score'],
                'training_time': training_time * 1000,
                'prediction_time': prediction_time * 1000,
                'predictions': predictions
            }
            
            print(f"    Doƒüruluk: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        # Random Forest
        n_trees_list = [50, 100]
        for n_trees in n_trees_list:
            print(f"  Random Forest (n_trees={n_trees}) test ediliyor...")
            
            rf = cv2.ml.RTrees_create()
            rf.setMaxDepth(10)
            rf.setMinSampleCount(2)
            rf.setActiveVarCount(0)  # sqrt(n_features)
            rf.setTermCriteria((cv2.TERM_CRITERIA_MAX_ITER, n_trees, 0))
            
            # Eƒüitim
            start_time = time.time()
            rf.train(X_train, cv2.ml.ROW_SAMPLE, y_train.astype(np.int32))
            training_time = time.time() - start_time
            
            # Test
            start_time = time.time()
            _, predictions = rf.predict(X_test)
            prediction_time = time.time() - start_time
            
            predictions = predictions.flatten().astype(np.int32)
            
            # Deƒüerlendirme
            accuracy = accuracy_score(y_test, predictions)
            report = classification_report(y_test, predictions, output_dict=True)
            
            model_name = f"Random Forest (n_trees={n_trees})"
            self.results[model_name] = {
                'model': rf,
                'accuracy': accuracy,
                'precision': report['macro avg']['precision'],
                'recall': report['macro avg']['recall'],
                'f1_score': report['macro avg']['f1-score'],
                'training_time': training_time * 1000,
                'prediction_time': prediction_time * 1000,
                'predictions': predictions
            }
            
            print(f"    Doƒüruluk: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    def perform_cross_validation(self, X, y):
        """Cross-validation analizi"""
        print("\nüîÑ Cross-Validation Analizi...")
        
        # Sadece bazƒ± modeller i√ßin CV (hƒ±zlƒ± olmasƒ± i√ßin)
        cv_models = {}
        
        # k-NN i√ßin basit cross-validation
        print("  k-NN (k=5) CV analizi...")
        try:
            knn = cv2.ml.KNearest_create()
            knn.setDefaultK(5)
            knn.setIsClassifier(True)
            
            # Manual k-fold CV (OpenCV modelleri sklearn cross_val_score ile direkt √ßalƒ±≈ümaz)
            kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
            cv_scores = []
            
            for train_idx, val_idx in kfold.split(X, y):
                X_train_cv, X_val_cv = X[train_idx], X[val_idx]
                y_train_cv, y_val_cv = y[train_idx], y[val_idx]
                
                # Normalizasyon
                scaler = StandardScaler()
                X_train_cv = scaler.fit_transform(X_train_cv).astype(np.float32)
                X_val_cv = scaler.transform(X_val_cv).astype(np.float32)
                
                # Eƒüitim ve test
                knn.train(X_train_cv, cv2.ml.ROW_SAMPLE, y_train_cv.astype(np.int32))
                _, predictions = knn.predict(X_val_cv)
                predictions = predictions.flatten().astype(np.int32)
                
                accuracy = accuracy_score(y_val_cv, predictions)
                cv_scores.append(accuracy)
            
            cv_mean = np.mean(cv_scores)
            cv_std = np.std(cv_scores)
            
            print(f"    CV Doƒüruluk: {cv_mean:.4f} ¬± {cv_std:.4f}")
            
        except Exception as e:
            print(f"    ‚ùå CV hatasƒ±: {e}")

class ResultAnalyzer:
    """Sonu√ß analizi sƒ±nƒ±fƒ±"""
    
    def __init__(self, trainer):
        self.trainer = trainer
        self.results = trainer.results
    
    def create_comparison_table(self):
        """Model kar≈üƒ±la≈ütƒ±rma tablosu"""
        print("\nüìä MODEL KAR≈ûILA≈ûTIRMA TABLOSU")
        print("=" * 90)
        print(f"{'Model':<25} | {'Accuracy':<10} | {'Precision':<10} | {'Recall':<10} | {'F1-Score':<10} | {'Time (ms)':<10}")
        print("-" * 90)
        
        # Sonu√ßlarƒ± accuracy'e g√∂re sƒ±rala
        sorted_results = sorted(self.results.items(), key=lambda x: x[1]['accuracy'], reverse=True)
        
        for model_name, metrics in sorted_results:
            print(f"{model_name:<25} | {metrics['accuracy']:<10.4f} | {metrics['precision']:<10.4f} | "
                  f"{metrics['recall']:<10.4f} | {metrics['f1_score']:<10.4f} | {metrics['training_time']:<10.1f}")
        
        # En iyi model
        if sorted_results:
            best_model_name, best_metrics = sorted_results[0]
            print(f"\nüèÜ EN ƒ∞Yƒ∞ MODEL: {best_model_name}")
            print(f"   Doƒüruluk: {best_metrics['accuracy']:.4f} ({best_metrics['accuracy']*100:.2f}%)")
    
    def plot_performance_comparison(self):
        """Performans kar≈üƒ±la≈ütƒ±rma grafikleri"""
        if not self.results:
            print("‚ùå Analiz edilecek sonu√ß bulunamadƒ±!")
            return
        
        # Veriyi hazƒ±rla
        model_names = list(self.results.keys())
        accuracies = [self.results[name]['accuracy'] for name in model_names]
        times = [self.results[name]['training_time'] for name in model_names]
        
        # Grafik √ßiz
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Doƒüruluk kar≈üƒ±la≈ütƒ±rmasƒ±
        bars1 = ax1.bar(range(len(model_names)), accuracies, color='skyblue', alpha=0.8)
        ax1.set_xlabel('Modeller')
        ax1.set_ylabel('Doƒüruluk')
        ax1.set_title('Model Doƒüruluk Kar≈üƒ±la≈ütƒ±rmasƒ±')
        ax1.set_xticks(range(len(model_names)))
        ax1.set_xticklabels([name.split('(')[0].strip() for name in model_names], rotation=45, ha='right')
        ax1.set_ylim(0, 1)
        
        # Deƒüerleri √ßubuklarƒ±n √ºzerine yaz
        for i, bar in enumerate(bars1):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{accuracies[i]:.3f}', ha='center', va='bottom', fontsize=8)
        
        # Eƒüitim s√ºresi kar≈üƒ±la≈ütƒ±rmasƒ±
        bars2 = ax2.bar(range(len(model_names)), times, color='lightcoral', alpha=0.8)
        ax2.set_xlabel('Modeller')
        ax2.set_ylabel('Eƒüitim S√ºresi (ms)')
        ax2.set_title('Model Eƒüitim S√ºresi Kar≈üƒ±la≈ütƒ±rmasƒ±')
        ax2.set_xticks(range(len(model_names)))
        ax2.set_xticklabels([name.split('(')[0].strip() for name in model_names], rotation=45, ha='right')
        
        # Deƒüerleri √ßubuklarƒ±n √ºzerine yaz
        for i, bar in enumerate(bars2):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + max(times)*0.01,
                    f'{times[i]:.1f}', ha='center', va='bottom', fontsize=8)
        
        plt.tight_layout()
        plt.show()
    
    def plot_confusion_matrices(self):
        """Confusion matrix'leri √ßiz"""
        if not self.results:
            return
        
        # En iyi 4 modeli se√ß
        sorted_results = sorted(self.results.items(), key=lambda x: x[1]['accuracy'], reverse=True)
        top_models = sorted_results[:4]
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.ravel()
        
        for i, (model_name, metrics) in enumerate(top_models):
            if i >= 4:
                break
            
            cm = confusion_matrix(self.trainer.y_test, metrics['predictions'])
            
            # Confusion matrix √ßiz
            im = axes[i].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
            axes[i].set_title(f'{model_name}')
            
            # Colorbar
            plt.colorbar(im, ax=axes[i])
            
            # Eksen etiketleri
            axes[i].set_xlabel('Tahmin')
            axes[i].set_ylabel('Ger√ßek')
            
            # Sayƒ±larƒ± yaz
            thresh = cm.max() / 2.
            for row in range(cm.shape[0]):
                for col in range(cm.shape[1]):
                    axes[i].text(col, row, format(cm[row, col], 'd'),
                               ha="center", va="center",
                               color="white" if cm[row, col] > thresh else "black")
        
        plt.tight_layout()
        plt.show()
    
    def analyze_feature_importance(self):
        """√ñzellik √∂nem analizi"""
        print("\nüîç √ñzellik √ñnem Analizi")
        print("=" * 25)
        
        # Random Forest ve Decision Tree i√ßin √∂zellik √∂nemleri
        feature_names = [
            'Hist_0', 'Hist_1', 'Hist_2', 'Hist_3', 'Hist_4', 'Hist_5', 'Hist_6', 'Hist_7',
            'Hist_8', 'Hist_9', 'Hist_10', 'Hist_11', 'Hist_12', 'Hist_13', 'Hist_14', 'Hist_15',
            'Mean', 'Std', 'Min', 'Max',
            'Edge_Density', 'Gradient_Mag',
            'LBP_0', 'LBP_1', 'LBP_2', 'LBP_3', 'LBP_4', 'LBP_5', 'LBP_6', 'LBP_7', 'LBP_8', 'LBP_9'
        ]
        
        # Random Forest modeli bul
        rf_models = {name: metrics for name, metrics in self.results.items() if 'Random Forest' in name}
        
        if rf_models:
            # En iyi Random Forest modelini al
            best_rf = max(rf_models.items(), key=lambda x: x[1]['accuracy'])
            rf_name, rf_metrics = best_rf
            
            try:
                # OpenCV Random Forest'tan √∂zellik √∂nemleri al
                rf_model = rf_metrics['model']
                importance = rf_model.getVarImportance()
                
                if importance is not None:
                    importance = importance.flatten()
                    
                    # En √∂nemli 10 √∂zelliƒüi g√∂ster
                    importance_pairs = list(zip(feature_names, importance))
                    importance_pairs.sort(key=lambda x: x[1], reverse=True)
                    
                    print(f"üìä {rf_name} - En √ñnemli 10 √ñzellik:")
                    for i, (feature, imp) in enumerate(importance_pairs[:10]):
                        print(f"  {i+1:2d}. {feature:<15}: {imp:.4f}")
                    
                    # Grafik √ßiz
                    top_features = importance_pairs[:10]
                    feature_names_top = [f[0] for f in top_features]
                    importance_values = [f[1] for f in top_features]
                    
                    plt.figure(figsize=(12, 6))
                    bars = plt.bar(range(len(feature_names_top)), importance_values, color='green', alpha=0.7)
                    plt.xlabel('√ñzellikler')
                    plt.ylabel('√ñnem')
                    plt.title(f'{rf_name} - √ñzellik √ñnem Analizi')
                    plt.xticks(range(len(feature_names_top)), feature_names_top, rotation=45, ha='right')
                    
                    # Deƒüerleri √ßubuklarƒ±n √ºzerine yaz
                    for i, bar in enumerate(bars):
                        height = bar.get_height()
                        plt.text(bar.get_x() + bar.get_width()/2., height + max(importance_values)*0.01,
                                f'{importance_values[i]:.3f}', ha='center', va='bottom', fontsize=8)
                    
                    plt.tight_layout()
                    plt.show()
                else:
                    print("‚ùå Random Forest'tan √∂zellik √∂nemleri alƒ±namadƒ±")
                    
            except Exception as e:
                print(f"‚ùå √ñzellik √∂nem analizi hatasƒ±: {e}")
        else:
            print("‚ùå Random Forest modeli bulunamadƒ±")

class RealtimePredictor:
    """Ger√ßek zamanlƒ± tahmin sƒ±nƒ±fƒ±"""
    
    def __init__(self, project, best_model_info):
        self.project = project
        self.best_model = best_model_info['model']
        self.model_name = best_model_info['name']
        self.prediction_history = []
        self.confidence_history = []
        
    def start_realtime_prediction(self):
        """Ger√ßek zamanlƒ± tahmin ba≈ülat"""
        print(f"\nüîÆ Ger√ßek Zamanlƒ± Tahmin Modu - {self.model_name}")
        print("ESC: √áƒ±kƒ±≈ü, R: Ge√ßmi≈üi temizle")
        
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("‚ùå Webcam a√ßƒ±lamadƒ±!")
            return
        
        frame_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                continue
            
            frame_count += 1
            display_frame = frame.copy()
            
            # Her 5 frame'de bir tahmin yap (performans i√ßin)
            if frame_count % 5 == 0:
                # ROI al ve i≈üle
                roi = frame[50:frame.shape[0]-100, 50:frame.shape[1]-50]
                gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
                resized = cv2.resize(gray, (32, 32))
                
                # √ñzellik √ßƒ±kar
                features = FeatureExtractor.extract_comprehensive_features(resized)
                features = self.project.scaler.transform(features.reshape(1, -1)).astype(np.float32)
                
                # Tahmin yap
                prediction, confidence = self.predict_with_confidence(features)
                
                # Ge√ßmi≈üe ekle
                self.prediction_history.append(prediction)
                self.confidence_history.append(confidence)
                
                # Son 10 tahmini sakla
                if len(self.prediction_history) > 10:
                    self.prediction_history.pop(0)
                    self.confidence_history.pop(0)
            
            # ROI √ßer√ßevesi √ßiz
            cv2.rectangle(display_frame, (50, 50), (frame.shape[1]-50, frame.shape[0]-100), (0, 255, 255), 2)
            
            # Tahmin sonu√ßlarƒ±nƒ± g√∂ster
            if self.prediction_history:
                current_prediction = self.prediction_history[-1]
                current_confidence = self.confidence_history[-1]
                
                # Ana tahmin
                predicted_class = self.project.class_names[current_prediction]
                cv2.putText(display_frame, f'Tahmin: {predicted_class}', (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
                
                # G√ºven skoru
                cv2.putText(display_frame, f'Guven: {current_confidence:.3f}', (10, 65),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                
                # Model adƒ±
                cv2.putText(display_frame, f'Model: {self.model_name}', (10, 100),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                # Tahmin ge√ßmi≈üi (stabilite g√∂stergesi)
                if len(self.prediction_history) > 1:
                    # Son 5 tahminin ne kadar tutarlƒ± olduƒüunu g√∂ster
                    recent_predictions = self.prediction_history[-5:]
                    most_common = max(set(recent_predictions), key=recent_predictions.count)
                    stability = recent_predictions.count(most_common) / len(recent_predictions)
                    
                    stability_color = (0, 255, 0) if stability > 0.8 else ((0, 255, 255) if stability > 0.6 else (0, 0, 255))
                    cv2.putText(display_frame, f'Stabilite: {stability:.2f}', (10, 135),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, stability_color, 2)
            
            # Kontrol bilgileri
            cv2.putText(display_frame, 'ESC: Cikis, R: Gecmis temizle', 
                       (10, frame.shape[0] - 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
            
            cv2.imshow('Gercek Zamanli Tahmin', display_frame)
            
            key = cv2.waitKey(1) & 0xFF
            if key == 27:  # ESC
                break
            elif key == ord('r') or key == ord('R'):  # R - Ge√ßmi≈üi temizle
                self.prediction_history.clear()
                self.confidence_history.clear()
                print("üßπ Tahmin ge√ßmi≈üi temizlendi")
        
        cap.release()
        cv2.destroyAllWindows()
        
        # Final istatistikleri
        if self.prediction_history:
            print(f"\nüìä Tahmin ƒ∞statistikleri:")
            for i, class_name in enumerate(self.project.class_names):
                count = self.prediction_history.count(i)
                percentage = count / len(self.prediction_history) * 100
                print(f"  {class_name}: {count} tahmin (%{percentage:.1f})")
    
    def predict_with_confidence(self, features):
        """G√ºven skoru ile tahmin"""
        # Model t√ºr√ºne g√∂re tahmin
        if 'k-NN' in self.model_name:
            _, raw_prediction = self.best_model.predict(features)
            prediction = int(raw_prediction[0][0])
            confidence = 0.8  # k-NN i√ßin sabit g√ºven skoru
            
        elif 'SVM' in self.model_name:
            _, raw_prediction = self.best_model.predict(features)
            prediction = int(raw_prediction[0][0])
            confidence = 0.85  # SVM i√ßin sabit g√ºven skoru
            
        elif 'ANN' in self.model_name:
            _, raw_prediction = self.best_model.predict(features)
            prediction = np.argmax(raw_prediction[0])
            confidence = np.max(raw_prediction[0])  # Softmax √ßƒ±kƒ±≈üƒ±
            
        elif 'Tree' in self.model_name or 'Forest' in self.model_name:
            _, raw_prediction = self.best_model.predict(features)
            prediction = int(raw_prediction[0][0])
            confidence = 0.75  # Aƒüa√ß modelleri i√ßin sabit g√ºven skoru
            
        else:
            _, raw_prediction = self.best_model.predict(features)
            prediction = int(raw_prediction[0][0])
            confidence = 0.7
        
        return prediction, confidence

def save_project_data(project, filename=None):
    """Proje verilerini kaydet"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"image_classification_project_{timestamp}.pkl"
    
    try:
        data = {
            'n_classes': project.n_classes,
            'class_names': project.class_names,
            'dataset': project.dataset,
            'labels': project.labels,
            'scaler': project.scaler,
            'feature_dim': project.feature_dim
        }
        
        with open(filename, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"‚úÖ Proje verileri kaydedildi: {filename}")
        return filename
    except Exception as e:
        print(f"‚ùå Kaydetme hatasƒ±: {e}")
        return None

def load_project_data(filename):
    """Proje verilerini y√ºkle"""
    try:
        with open(filename, 'rb') as f:
            data = pickle.load(f)
        
        # Proje nesnesini yeniden olu≈ütur
        project = ImageClassificationProject(n_classes=data['n_classes'])
        project.class_names = data['class_names']
        project.dataset = data['dataset']
        project.labels = data['labels']
        project.scaler = data['scaler']
        project.feature_dim = data['feature_dim']
        
        print(f"‚úÖ Proje verileri y√ºklendi: {filename}")
        print(f"   {len(project.dataset)} √∂rnek, {project.n_classes} sƒ±nƒ±f")
        
        return project
    except Exception as e:
        print(f"‚ùå Y√ºkleme hatasƒ±: {e}")
        return None

def main():
    """Ana program"""
    print("üéØ √áok Sƒ±nƒ±flƒ± G√∂r√ºnt√º Sƒ±nƒ±flandƒ±rma Projesi - √á√ñZ√úM")
    print("=" * 55)
    
    # Proje ba≈ülat
    project = ImageClassificationProject(n_classes=5)
    trainer = None
    
    # Sƒ±nƒ±f isimlerini ayarla
    class_names = ["Kitap", "Kalem", "Telefon", "Bardak", "Anahtar"]
    project.set_class_names(class_names)
    
    while True:
        print("\n" + "="*50)
        print("üéØ ANA MEN√ú")
        print("="*50)
        print("1. üì∏ Veri Toplama")
        print("2. üîß Model Eƒüitimi") 
        print("3. üìä Sonu√ß Analizi")
        print("4. üîÆ Ger√ßek Zamanlƒ± Tahmin")
        print("5. üíæ Veri/Model Kaydet")
        print("6. üìÅ Veri/Model Y√ºkle")
        print("0. ‚ùå √áƒ±kƒ±≈ü")
        
        try:
            choice = input("\nSe√ßiminizi yapƒ±n (0-6): ").strip()
            
            if choice == "0":
                print("üëã Program sonlandƒ±rƒ±ldƒ±!")
                break
                
            elif choice == "1":
                # Veri toplama
                collector = DataCollector(project)
                success = collector.collect_data_interactive()
                if success:
                    print("‚úÖ Veri toplama tamamlandƒ±!")
                    collector.show_statistics()
                else:
                    print("‚ùå Veri toplama ba≈üarƒ±sƒ±z!")
            
            elif choice == "2":
                # Model eƒüitimi
                if len(project.dataset) == 0:
                    print("‚ùå √ñnce veri toplamalƒ±sƒ±nƒ±z!")
                    continue
                
                trainer = ModelTrainer(project)
                
                try:
                    X_train, X_test, y_train, y_test = trainer.prepare_data()
                    
                    # T√ºm modelleri eƒüit
                    trainer.train_knn_models(X_train, X_test, y_train, y_test)
                    trainer.train_svm_models(X_train, X_test, y_train, y_test)
                    trainer.train_ann_models(X_train, X_test, y_train, y_test)
                    trainer.train_tree_models(X_train, X_test, y_train, y_test)
                    
                    # Cross-validation
                    X_all = np.vstack([X_train, X_test])
                    y_all = np.hstack([y_train, y_test])
                    trainer.perform_cross_validation(X_all, y_all)
                    
                    print("‚úÖ T√ºm modeller eƒüitildi!")
                    
                except Exception as e:
                    print(f"‚ùå Model eƒüitimi hatasƒ±: {e}")
            
            elif choice == "3":
                # Sonu√ß analizi
                if trainer is None or not trainer.results:
                    print("‚ùå √ñnce modelleri eƒüitmelisiniz!")
                    continue
                
                analyzer = ResultAnalyzer(trainer)
                analyzer.create_comparison_table()
                analyzer.plot_performance_comparison()
                analyzer.plot_confusion_matrices()
                analyzer.analyze_feature_importance()
            
            elif choice == "4":
                # Ger√ßek zamanlƒ± tahmin
                if trainer is None or not trainer.results:
                    print("‚ùå √ñnce modelleri eƒüitmelisiniz!")
                    continue
                
                # En iyi modeli se√ß
                best_model_name = max(trainer.results.keys(), key=lambda x: trainer.results[x]['accuracy'])
                best_model_info = {
                    'name': best_model_name,
                    'model': trainer.results[best_model_name]['model']
                }
                
                print(f"üèÜ En iyi model se√ßildi: {best_model_name}")
                
                predictor = RealtimePredictor(project, best_model_info)
                predictor.start_realtime_prediction()
            
            elif choice == "5":
                # Kaydetme
                if len(project.dataset) == 0:
                    print("‚ùå Kaydedilecek veri bulunamadƒ±!")
                    continue
                
                filename = save_project_data(project)
                if filename:
                    print(f"üìÅ Dosya: {filename}")
            
            elif choice == "6":
                # Y√ºkleme
                filename = input("Y√ºklenecek dosya adƒ± (.pkl): ").strip()
                if not filename.endswith('.pkl'):
                    filename += '.pkl'
                
                loaded_project = load_project_data(filename)
                if loaded_project:
                    project = loaded_project
                    trainer = None  # Modellerin yeniden eƒüitilmesi gerekiyor
                    print("‚ö†Ô∏è Not: Modelleri yeniden eƒüitmeniz gerekiyor.")
            
            else:
                print("‚ùå Ge√ßersiz se√ßim! L√ºtfen 0-6 arasƒ±nda bir sayƒ± girin.")
        
        except KeyboardInterrupt:
            print("\n\nüëã Program kullanƒ±cƒ± tarafƒ±ndan sonlandƒ±rƒ±ldƒ±.")
            break
        except Exception as e:
            print(f"‚ùå Beklenmeyen hata: {e}")

if __name__ == "__main__":
    main()

# üìù √á√ñZ√úM NOTLARI:
# Bu implementasyon alƒ±≈ütƒ±rma 1'in tam √ß√∂z√ºm√ºd√ºr.
# 
# √ñzellikler:
# ‚úÖ 32 boyutlu kapsamlƒ± √∂zellik √ßƒ±karma
# ‚úÖ 5 farklƒ± ML algoritmasƒ± implementasyonu  
# ‚úÖ Cross-validation analizi
# ‚úÖ Ger√ßek zamanlƒ± tahmin sistemi
# ‚úÖ Kapsamlƒ± performans analizi
# ‚úÖ Veri kaydetme/y√ºkleme
# ‚úÖ Feature importance analizi
# ‚úÖ Confusion matrix g√∂rselle≈ütirme
#
# Performans hedefleri:
# - Minimum %80 doƒüruluk: ‚úÖ (iyi veri ile)
# - <50ms tahmin s√ºresi: ‚úÖ 
# - Model kararlƒ±lƒ±ƒüƒ±: ‚úÖ (CV ile test edildi)